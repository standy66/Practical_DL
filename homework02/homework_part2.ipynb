{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3, 32, 32) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from cifar import load_cifar10\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_cifar10(\"cifar_data\")\n",
    "class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): LeakyReLU(0.01)\n",
       "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (5): LeakyReLU(0.01)\n",
       "  (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (9): LeakyReLU(0.01)\n",
       "  (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (12): LeakyReLU(0.01)\n",
       "  (13): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (16): LeakyReLU(0.01)\n",
       "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (19): LeakyReLU(0.01)\n",
       "  (20): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (21): Flatten(\n",
       "  )\n",
       "  (22): Dropout(p=0.5)\n",
       "  (23): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "  (24): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (25): LeakyReLU(0.01)\n",
       "  (26): Dropout(p=0.5)\n",
       "  (27): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "  (28): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (29): LeakyReLU(0.01)\n",
       "  (30): Dropout(p=0.5)\n",
       "  (31): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=(5, 5), padding=2, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.MaxPool2d((3, 3), stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.MaxPool2d((3, 3), stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.MaxPool2d((3, 3), stride=2, padding=1),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 2048, bias=False),\n",
    "    nn.BatchNorm2d(2048),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.Dropout(),\n",
    "    nn.Linear(2048, 1024, bias=False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.LeakyReLU(),\n",
    "    \n",
    "    nn.Dropout(),\n",
    "    nn.Linear(1024, 10)\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Training __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, initial_lr, epoch):\n",
    "    lr = initial_lr * (0.9 ** (epoch // 3))\n",
    "    print(\"New learning rate: {}\".format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "        \n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New learning rate: 0.01\n",
      "Epoch 1 of 100 took 11.560s\n",
      "  training loss (in-iteration): \t1.411481\n",
      "  validation accuracy: \t\t\t62.23 %\n",
      "New learning rate: 0.01\n",
      "Epoch 2 of 100 took 11.079s\n",
      "  training loss (in-iteration): \t0.943350\n",
      "  validation accuracy: \t\t\t54.80 %\n",
      "New learning rate: 0.01\n",
      "Epoch 3 of 100 took 11.111s\n",
      "  training loss (in-iteration): \t0.771843\n",
      "  validation accuracy: \t\t\t75.34 %\n",
      "New learning rate: 0.009000000000000001\n",
      "Epoch 4 of 100 took 11.138s\n",
      "  training loss (in-iteration): \t0.654050\n",
      "  validation accuracy: \t\t\t76.00 %\n",
      "New learning rate: 0.009000000000000001\n",
      "Epoch 5 of 100 took 11.120s\n",
      "  training loss (in-iteration): \t0.584165\n",
      "  validation accuracy: \t\t\t74.93 %\n",
      "New learning rate: 0.009000000000000001\n",
      "Epoch 6 of 100 took 11.157s\n",
      "  training loss (in-iteration): \t0.516268\n",
      "  validation accuracy: \t\t\t79.64 %\n",
      "New learning rate: 0.008100000000000001\n",
      "Epoch 7 of 100 took 11.125s\n",
      "  training loss (in-iteration): \t0.453549\n",
      "  validation accuracy: \t\t\t82.41 %\n",
      "New learning rate: 0.008100000000000001\n",
      "Epoch 8 of 100 took 11.165s\n",
      "  training loss (in-iteration): \t0.405629\n",
      "  validation accuracy: \t\t\t82.29 %\n",
      "New learning rate: 0.008100000000000001\n",
      "Epoch 9 of 100 took 11.270s\n",
      "  training loss (in-iteration): \t0.369875\n",
      "  validation accuracy: \t\t\t82.59 %\n",
      "New learning rate: 0.007290000000000001\n",
      "Epoch 10 of 100 took 11.326s\n",
      "  training loss (in-iteration): \t0.325175\n",
      "  validation accuracy: \t\t\t81.32 %\n",
      "New learning rate: 0.007290000000000001\n",
      "Epoch 11 of 100 took 11.340s\n",
      "  training loss (in-iteration): \t0.290322\n",
      "  validation accuracy: \t\t\t84.11 %\n",
      "New learning rate: 0.007290000000000001\n",
      "Epoch 12 of 100 took 11.401s\n",
      "  training loss (in-iteration): \t0.265557\n",
      "  validation accuracy: \t\t\t83.89 %\n",
      "New learning rate: 0.006561\n",
      "Epoch 13 of 100 took 11.443s\n",
      "  training loss (in-iteration): \t0.223013\n",
      "  validation accuracy: \t\t\t83.91 %\n",
      "New learning rate: 0.006561\n",
      "Epoch 14 of 100 took 11.437s\n",
      "  training loss (in-iteration): \t0.201884\n",
      "  validation accuracy: \t\t\t84.47 %\n",
      "New learning rate: 0.006561\n",
      "Epoch 15 of 100 took 11.440s\n",
      "  training loss (in-iteration): \t0.171634\n",
      "  validation accuracy: \t\t\t82.45 %\n",
      "New learning rate: 0.005904900000000001\n",
      "Epoch 16 of 100 took 11.426s\n",
      "  training loss (in-iteration): \t0.150463\n",
      "  validation accuracy: \t\t\t84.27 %\n",
      "New learning rate: 0.005904900000000001\n",
      "Epoch 17 of 100 took 11.465s\n",
      "  training loss (in-iteration): \t0.133710\n",
      "  validation accuracy: \t\t\t84.69 %\n",
      "New learning rate: 0.005904900000000001\n",
      "Epoch 18 of 100 took 11.507s\n",
      "  training loss (in-iteration): \t0.121159\n",
      "  validation accuracy: \t\t\t84.94 %\n",
      "New learning rate: 0.00531441\n",
      "Epoch 19 of 100 took 11.510s\n",
      "  training loss (in-iteration): \t0.098768\n",
      "  validation accuracy: \t\t\t85.17 %\n",
      "New learning rate: 0.00531441\n",
      "Epoch 20 of 100 took 11.466s\n",
      "  training loss (in-iteration): \t0.085702\n",
      "  validation accuracy: \t\t\t84.75 %\n",
      "New learning rate: 0.00531441\n",
      "Epoch 21 of 100 took 11.486s\n",
      "  training loss (in-iteration): \t0.076235\n",
      "  validation accuracy: \t\t\t85.47 %\n",
      "New learning rate: 0.004782969000000001\n",
      "Epoch 22 of 100 took 11.463s\n",
      "  training loss (in-iteration): \t0.063719\n",
      "  validation accuracy: \t\t\t83.87 %\n",
      "New learning rate: 0.004782969000000001\n",
      "Epoch 23 of 100 took 11.402s\n",
      "  training loss (in-iteration): \t0.059300\n",
      "  validation accuracy: \t\t\t86.25 %\n",
      "New learning rate: 0.004782969000000001\n",
      "Epoch 24 of 100 took 11.487s\n",
      "  training loss (in-iteration): \t0.052367\n",
      "  validation accuracy: \t\t\t84.60 %\n",
      "New learning rate: 0.004304672100000001\n",
      "Epoch 25 of 100 took 11.429s\n",
      "  training loss (in-iteration): \t0.045444\n",
      "  validation accuracy: \t\t\t85.68 %\n",
      "New learning rate: 0.004304672100000001\n",
      "Epoch 26 of 100 took 11.382s\n",
      "  training loss (in-iteration): \t0.040037\n",
      "  validation accuracy: \t\t\t85.82 %\n",
      "New learning rate: 0.004304672100000001\n",
      "Epoch 27 of 100 took 11.319s\n",
      "  training loss (in-iteration): \t0.036176\n",
      "  validation accuracy: \t\t\t85.72 %\n",
      "New learning rate: 0.003874204890000001\n",
      "Epoch 28 of 100 took 11.336s\n",
      "  training loss (in-iteration): \t0.029957\n",
      "  validation accuracy: \t\t\t86.30 %\n",
      "New learning rate: 0.003874204890000001\n",
      "Epoch 29 of 100 took 11.341s\n",
      "  training loss (in-iteration): \t0.027377\n",
      "  validation accuracy: \t\t\t86.45 %\n",
      "New learning rate: 0.003874204890000001\n",
      "Epoch 30 of 100 took 11.308s\n",
      "  training loss (in-iteration): \t0.024117\n",
      "  validation accuracy: \t\t\t86.20 %\n",
      "New learning rate: 0.003486784401000001\n",
      "Epoch 31 of 100 took 11.251s\n",
      "  training loss (in-iteration): \t0.020507\n",
      "  validation accuracy: \t\t\t86.48 %\n",
      "New learning rate: 0.003486784401000001\n",
      "Epoch 32 of 100 took 11.208s\n",
      "  training loss (in-iteration): \t0.017936\n",
      "  validation accuracy: \t\t\t86.66 %\n",
      "New learning rate: 0.003486784401000001\n",
      "Epoch 33 of 100 took 11.155s\n",
      "  training loss (in-iteration): \t0.014617\n",
      "  validation accuracy: \t\t\t86.47 %\n",
      "New learning rate: 0.0031381059609000006\n",
      "Epoch 34 of 100 took 11.177s\n",
      "  training loss (in-iteration): \t0.015445\n",
      "  validation accuracy: \t\t\t86.74 %\n",
      "New learning rate: 0.0031381059609000006\n",
      "Epoch 35 of 100 took 11.173s\n",
      "  training loss (in-iteration): \t0.012693\n",
      "  validation accuracy: \t\t\t86.45 %\n",
      "New learning rate: 0.0031381059609000006\n",
      "Epoch 36 of 100 took 11.121s\n",
      "  training loss (in-iteration): \t0.013451\n",
      "  validation accuracy: \t\t\t86.55 %\n",
      "New learning rate: 0.0028242953648100013\n",
      "Epoch 37 of 100 took 11.121s\n",
      "  training loss (in-iteration): \t0.012840\n",
      "  validation accuracy: \t\t\t86.68 %\n",
      "New learning rate: 0.0028242953648100013\n",
      "Epoch 38 of 100 took 11.098s\n",
      "  training loss (in-iteration): \t0.010277\n",
      "  validation accuracy: \t\t\t86.46 %\n",
      "New learning rate: 0.0028242953648100013\n",
      "Epoch 39 of 100 took 11.100s\n",
      "  training loss (in-iteration): \t0.011616\n",
      "  validation accuracy: \t\t\t86.62 %\n",
      "New learning rate: 0.002541865828329001\n",
      "Epoch 40 of 100 took 11.093s\n",
      "  training loss (in-iteration): \t0.010036\n",
      "  validation accuracy: \t\t\t86.94 %\n",
      "New learning rate: 0.002541865828329001\n",
      "Epoch 41 of 100 took 11.090s\n",
      "  training loss (in-iteration): \t0.008220\n",
      "  validation accuracy: \t\t\t86.87 %\n",
      "New learning rate: 0.002541865828329001\n",
      "Epoch 42 of 100 took 11.097s\n",
      "  training loss (in-iteration): \t0.008089\n",
      "  validation accuracy: \t\t\t87.31 %\n",
      "New learning rate: 0.002287679245496101\n",
      "Epoch 43 of 100 took 11.088s\n",
      "  training loss (in-iteration): \t0.006943\n",
      "  validation accuracy: \t\t\t86.99 %\n",
      "New learning rate: 0.002287679245496101\n",
      "Epoch 44 of 100 took 11.096s\n",
      "  training loss (in-iteration): \t0.005896\n",
      "  validation accuracy: \t\t\t87.25 %\n",
      "New learning rate: 0.002287679245496101\n",
      "Epoch 45 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.005864\n",
      "  validation accuracy: \t\t\t86.85 %\n",
      "New learning rate: 0.002058911320946491\n",
      "Epoch 46 of 100 took 11.097s\n",
      "  training loss (in-iteration): \t0.005275\n",
      "  validation accuracy: \t\t\t87.14 %\n",
      "New learning rate: 0.002058911320946491\n",
      "Epoch 47 of 100 took 11.093s\n",
      "  training loss (in-iteration): \t0.005320\n",
      "  validation accuracy: \t\t\t87.06 %\n",
      "New learning rate: 0.002058911320946491\n",
      "Epoch 48 of 100 took 11.094s\n",
      "  training loss (in-iteration): \t0.004847\n",
      "  validation accuracy: \t\t\t87.38 %\n",
      "New learning rate: 0.0018530201888518416\n",
      "Epoch 49 of 100 took 11.100s\n",
      "  training loss (in-iteration): \t0.005243\n",
      "  validation accuracy: \t\t\t86.76 %\n",
      "New learning rate: 0.0018530201888518416\n",
      "Epoch 50 of 100 took 11.077s\n",
      "  training loss (in-iteration): \t0.004261\n",
      "  validation accuracy: \t\t\t86.73 %\n",
      "New learning rate: 0.0018530201888518416\n",
      "Epoch 51 of 100 took 11.092s\n",
      "  training loss (in-iteration): \t0.004086\n",
      "  validation accuracy: \t\t\t86.88 %\n",
      "New learning rate: 0.0016677181699666576\n",
      "Epoch 52 of 100 took 11.085s\n",
      "  training loss (in-iteration): \t0.003854\n",
      "  validation accuracy: \t\t\t86.87 %\n",
      "New learning rate: 0.0016677181699666576\n",
      "Epoch 53 of 100 took 11.087s\n",
      "  training loss (in-iteration): \t0.004284\n",
      "  validation accuracy: \t\t\t87.05 %\n",
      "New learning rate: 0.0016677181699666576\n",
      "Epoch 54 of 100 took 11.083s\n",
      "  training loss (in-iteration): \t0.003462\n",
      "  validation accuracy: \t\t\t86.92 %\n",
      "New learning rate: 0.0015009463529699917\n",
      "Epoch 55 of 100 took 11.093s\n",
      "  training loss (in-iteration): \t0.003328\n",
      "  validation accuracy: \t\t\t87.33 %\n",
      "New learning rate: 0.0015009463529699917\n",
      "Epoch 56 of 100 took 11.077s\n",
      "  training loss (in-iteration): \t0.004073\n",
      "  validation accuracy: \t\t\t87.24 %\n",
      "New learning rate: 0.0015009463529699917\n",
      "Epoch 57 of 100 took 11.092s\n",
      "  training loss (in-iteration): \t0.003610\n",
      "  validation accuracy: \t\t\t87.31 %\n",
      "New learning rate: 0.0013508517176729928\n",
      "Epoch 58 of 100 took 11.086s\n",
      "  training loss (in-iteration): \t0.002906\n",
      "  validation accuracy: \t\t\t87.42 %\n",
      "New learning rate: 0.0013508517176729928\n",
      "Epoch 59 of 100 took 11.085s\n",
      "  training loss (in-iteration): \t0.003855\n",
      "  validation accuracy: \t\t\t87.20 %\n",
      "New learning rate: 0.0013508517176729928\n",
      "Epoch 60 of 100 took 11.090s\n",
      "  training loss (in-iteration): \t0.002934\n",
      "  validation accuracy: \t\t\t87.36 %\n",
      "New learning rate: 0.0012157665459056935\n",
      "Epoch 61 of 100 took 11.097s\n",
      "  training loss (in-iteration): \t0.002984\n",
      "  validation accuracy: \t\t\t87.19 %\n",
      "New learning rate: 0.0012157665459056935\n",
      "Epoch 62 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.002807\n",
      "  validation accuracy: \t\t\t87.49 %\n",
      "New learning rate: 0.0012157665459056935\n",
      "Epoch 63 of 100 took 11.087s\n",
      "  training loss (in-iteration): \t0.002665\n",
      "  validation accuracy: \t\t\t87.24 %\n",
      "New learning rate: 0.0010941898913151243\n",
      "Epoch 64 of 100 took 11.096s\n",
      "  training loss (in-iteration): \t0.002590\n",
      "  validation accuracy: \t\t\t87.23 %\n",
      "New learning rate: 0.0010941898913151243\n",
      "Epoch 65 of 100 took 11.096s\n",
      "  training loss (in-iteration): \t0.002641\n",
      "  validation accuracy: \t\t\t87.09 %\n",
      "New learning rate: 0.0010941898913151243\n",
      "Epoch 66 of 100 took 11.098s\n",
      "  training loss (in-iteration): \t0.002466\n",
      "  validation accuracy: \t\t\t87.35 %\n",
      "New learning rate: 0.0009847709021836117\n",
      "Epoch 67 of 100 took 11.093s\n",
      "  training loss (in-iteration): \t0.002962\n",
      "  validation accuracy: \t\t\t87.31 %\n",
      "New learning rate: 0.0009847709021836117\n",
      "Epoch 68 of 100 took 11.092s\n",
      "  training loss (in-iteration): \t0.002426\n",
      "  validation accuracy: \t\t\t87.35 %\n",
      "New learning rate: 0.0009847709021836117\n",
      "Epoch 69 of 100 took 11.088s\n",
      "  training loss (in-iteration): \t0.002168\n",
      "  validation accuracy: \t\t\t87.27 %\n",
      "New learning rate: 0.0008862938119652507\n",
      "Epoch 70 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.002394\n",
      "  validation accuracy: \t\t\t87.44 %\n",
      "New learning rate: 0.0008862938119652507\n",
      "Epoch 71 of 100 took 11.100s\n",
      "  training loss (in-iteration): \t0.002503\n",
      "  validation accuracy: \t\t\t87.21 %\n",
      "New learning rate: 0.0008862938119652507\n",
      "Epoch 72 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.002041\n",
      "  validation accuracy: \t\t\t87.33 %\n",
      "New learning rate: 0.0007976644307687256\n",
      "Epoch 73 of 100 took 11.100s\n",
      "  training loss (in-iteration): \t0.002022\n",
      "  validation accuracy: \t\t\t87.30 %\n",
      "New learning rate: 0.0007976644307687256\n",
      "Epoch 74 of 100 took 11.088s\n",
      "  training loss (in-iteration): \t0.001852\n",
      "  validation accuracy: \t\t\t87.34 %\n",
      "New learning rate: 0.0007976644307687256\n",
      "Epoch 75 of 100 took 11.099s\n",
      "  training loss (in-iteration): \t0.002216\n",
      "  validation accuracy: \t\t\t87.29 %\n",
      "New learning rate: 0.000717897987691853\n",
      "Epoch 76 of 100 took 11.099s\n",
      "  training loss (in-iteration): \t0.002272\n",
      "  validation accuracy: \t\t\t87.37 %\n",
      "New learning rate: 0.000717897987691853\n",
      "Epoch 77 of 100 took 11.085s\n",
      "  training loss (in-iteration): \t0.002358\n",
      "  validation accuracy: \t\t\t87.46 %\n",
      "New learning rate: 0.000717897987691853\n",
      "Epoch 78 of 100 took 11.090s\n",
      "  training loss (in-iteration): \t0.001626\n",
      "  validation accuracy: \t\t\t87.41 %\n",
      "New learning rate: 0.0006461081889226677\n",
      "Epoch 79 of 100 took 11.103s\n",
      "  training loss (in-iteration): \t0.002040\n",
      "  validation accuracy: \t\t\t87.38 %\n",
      "New learning rate: 0.0006461081889226677\n",
      "Epoch 80 of 100 took 11.094s\n",
      "  training loss (in-iteration): \t0.002094\n",
      "  validation accuracy: \t\t\t87.40 %\n",
      "New learning rate: 0.0006461081889226677\n",
      "Epoch 81 of 100 took 11.095s\n",
      "  training loss (in-iteration): \t0.001872\n",
      "  validation accuracy: \t\t\t87.29 %\n",
      "New learning rate: 0.000581497370030401\n",
      "Epoch 82 of 100 took 11.092s\n",
      "  training loss (in-iteration): \t0.001803\n",
      "  validation accuracy: \t\t\t87.44 %\n",
      "New learning rate: 0.000581497370030401\n",
      "Epoch 83 of 100 took 11.084s\n",
      "  training loss (in-iteration): \t0.001927\n",
      "  validation accuracy: \t\t\t87.59 %\n",
      "New learning rate: 0.000581497370030401\n",
      "Epoch 84 of 100 took 11.086s\n",
      "  training loss (in-iteration): \t0.001530\n",
      "  validation accuracy: \t\t\t87.45 %\n",
      "New learning rate: 0.000523347633027361\n",
      "Epoch 85 of 100 took 11.086s\n",
      "  training loss (in-iteration): \t0.001545\n",
      "  validation accuracy: \t\t\t87.33 %\n",
      "New learning rate: 0.000523347633027361\n",
      "Epoch 86 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.001743\n",
      "  validation accuracy: \t\t\t87.49 %\n",
      "New learning rate: 0.000523347633027361\n",
      "Epoch 87 of 100 took 11.092s\n",
      "  training loss (in-iteration): \t0.001463\n",
      "  validation accuracy: \t\t\t87.51 %\n",
      "New learning rate: 0.0004710128697246249\n",
      "Epoch 88 of 100 took 11.083s\n",
      "  training loss (in-iteration): \t0.001414\n",
      "  validation accuracy: \t\t\t87.36 %\n",
      "New learning rate: 0.0004710128697246249\n",
      "Epoch 89 of 100 took 11.112s\n",
      "  training loss (in-iteration): \t0.001731\n",
      "  validation accuracy: \t\t\t87.40 %\n",
      "New learning rate: 0.0004710128697246249\n",
      "Epoch 90 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.001713\n",
      "  validation accuracy: \t\t\t87.34 %\n",
      "New learning rate: 0.0004239115827521624\n",
      "Epoch 91 of 100 took 11.077s\n",
      "  training loss (in-iteration): \t0.001490\n",
      "  validation accuracy: \t\t\t87.49 %\n",
      "New learning rate: 0.0004239115827521624\n",
      "Epoch 92 of 100 took 11.082s\n",
      "  training loss (in-iteration): \t0.001392\n",
      "  validation accuracy: \t\t\t87.65 %\n",
      "New learning rate: 0.0004239115827521624\n",
      "Epoch 93 of 100 took 11.097s\n",
      "  training loss (in-iteration): \t0.001603\n",
      "  validation accuracy: \t\t\t87.57 %\n",
      "New learning rate: 0.00038152042447694615\n",
      "Epoch 94 of 100 took 11.087s\n",
      "  training loss (in-iteration): \t0.001708\n",
      "  validation accuracy: \t\t\t87.36 %\n",
      "New learning rate: 0.00038152042447694615\n",
      "Epoch 95 of 100 took 11.088s\n",
      "  training loss (in-iteration): \t0.001527\n",
      "  validation accuracy: \t\t\t87.40 %\n",
      "New learning rate: 0.00038152042447694615\n",
      "Epoch 96 of 100 took 11.100s\n",
      "  training loss (in-iteration): \t0.001371\n",
      "  validation accuracy: \t\t\t87.42 %\n",
      "New learning rate: 0.00034336838202925153\n",
      "Epoch 97 of 100 took 11.084s\n",
      "  training loss (in-iteration): \t0.001473\n",
      "  validation accuracy: \t\t\t87.38 %\n",
      "New learning rate: 0.00034336838202925153\n",
      "Epoch 98 of 100 took 11.096s\n",
      "  training loss (in-iteration): \t0.001595\n",
      "  validation accuracy: \t\t\t87.31 %\n",
      "New learning rate: 0.00034336838202925153\n",
      "Epoch 99 of 100 took 11.089s\n",
      "  training loss (in-iteration): \t0.001389\n",
      "  validation accuracy: \t\t\t87.51 %\n",
      "New learning rate: 0.00030903154382632634\n",
      "Epoch 100 of 100 took 11.095s\n",
      "  training loss (in-iteration): \t0.001560\n",
      "  validation accuracy: \t\t\t87.56 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "batch_size = 64  # number of samples processed in one SGD iteration\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(opt, 0.01, epoch)\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.data.cpu().numpy()[0])\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t87.25 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].cpu().data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1: model architectures\n",
    "\n",
    "|ALL-CONV-FC|ALL-CONV-AVGPOOL|POOL-CONV-FC|\n",
    "|-----------------------------------------|\n",
    "|conv32_5x5(stride=2, pad=1)|conv96_3x3(pad=1)|conv64_5x5(pad=2)|\n",
    "|conv64_3x3|conv96_3x3(pad=1)|conv64_3x3(pad=1)|\n",
    "|conv64_3x3|conv96_3x3(stride=2, pad=1)|max_pool_3x3_(stride=2, pad=1)|\n",
    "|conv128_3x3(stride=2, pad=1)|conv192_3x3(pad=1)|conv128_3x3(pad=1)|\n",
    "|conv256_3x3|conv192_3x3(pad=1)|conv128_3x3(pad=1)|\n",
    "|conv256_3x3|conv192_3x3(stride=2, pad=1)|max_pool_3x3_(stride=2, pad=1)|\n",
    "|conv512_3x3(stride=2, pad=1)|conv192_3x3(pad=1)|conv256_3x3(pad=1)|\n",
    "|fc_512|conv192_1x1(pad=1)|conv256_3x3(pad=1)|\n",
    "|fc_512|conv10_1x1(pad=1)|max_pool_3x3_(stride=2, pad=1)|\n",
    "|fc_10|global_avg_pool|fc_2048|\n",
    "|-|-|fc_1024|\n",
    "|-|-|fc_10|\n",
    "\n",
    "#### Table 2: Optimizer vs test accuracy for ALL-CONV-AVGPOOL model\n",
    "\n",
    "|Optimizer|ALL-CONV-AVGPOOL test accuracy|\n",
    "|-----------------------|\n",
    "|**SGD(momentum=0.9, lr=0.1)**|**78.67 %**|\n",
    "|Adam(lr=0.1)|73.44 %|\n",
    "\n",
    "#### Table 3: final results\n",
    "\n",
    "|Network|CIFAR-10 test accuracy, %|\n",
    "|------------------------------|\n",
    "|RESNET-18|64.32|\n",
    "|ALL-CONV-FC|78.65|\n",
    "|ALL-CONV-AVGPOOL|78.67|\n",
    "|Leaky POOL-CONV-FC|86.28|\n",
    "|**Leaky POOL-CONV-FC-Dropout**|**87.25**|\n",
    "|Leaky POOL-CONV-Dropout-FC-Dropout|86.64|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models consist of convolutional layers with *batch normalization* and some sort of *rectifier activation* with occasional *pooling* between the conv layers. The convolutional part of the network is followed by either *global average pooling* or *dense network*. In case of dense network batch normalization and dropout is also occasionally used between the fully connected layers. See **table 1** for reference.\n",
    "\n",
    "Also, we tried using resnet-18 model, but the final accuracy was subpar to those of other architectures, mainly because resnet model is optimized for 224x224 and to make it usable for 32x32 input we had to remove the final global pooling.\n",
    "\n",
    "All models were trained with the following learning rate scheduling: each third epoch learning rate was multiplied by a factor of 0.9. Learning rates were selected from the set [0.1, 0.01, 0.001] to achieve fastest convergence on a per model basis. Several different optimizers were tested, but tests have shown that momentum optimizer with momentum of 0.9 were the least prone to overfitting, so it was used for all models. See **table 2**.\n",
    "\n",
    "Batch normalization was used to improve stability of the training and also convergance speed. It is placed before the activation of each layer but the last one, which doesn't have any activation. The layer preceding the batch normalization had the bias turned off to reduce number of correlating parameters.\n",
    "\n",
    "ReLU activation was selected for every model except POOL-CONV-FC, where Leaky ReLU was used.\n",
    "\n",
    "We tried the all convolutional architechture from [Striving for Simplicity: The All Convolutional Net, Springenberg et al., 2014](https://arxiv.org/abs/1412.6806), see All-CNN-C, All-CNN-B from the paper. **We were unable to reproduce the results from the paper as in our case using max pooling instead of convolutional layers with strides greatly increased accuracy.** Having established that, we decided to use max pooling to continue our strive for the single best model, and so began the experiments on POOL-CONV-FC architechture.\n",
    "\n",
    "We used fully connected layers instead of global pooling after the convolutions because that allowed us to use dropout between them. Also, we tried using dropout between convolutional layers, but only the former proved to be useful in our tests, decrising the error rate by an absolute value of 1%.\n",
    "\n",
    "Our single best model Leaky POOL-CONV-FC-Dropout consisted of 3 blocks of pairs of convolutions followed by a max pooling layer. 3 fully connected layers with dropout between them follow the convolutional part of the net. Both batch normalization and dropout are proved to be useful in this model, because without them the final accuracy decreases slighly. For the results, see **table 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: torch.nn.Module logs and results\n",
    "\n",
    "```\n",
    "78.65 %\n",
    "lr = 0.01, 0.001\n",
    "ALL-CONV-FC\n",
    "Sequential(\n",
    "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
    "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (2): ReLU()\n",
    "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (5): ReLU()\n",
    "  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "  (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (8): ReLU()\n",
    "  (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (11): ReLU()\n",
    "  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (14): ReLU()\n",
    "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (17): ReLU()\n",
    "  (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (20): ReLU()\n",
    "  (21): Flatten(\n",
    "  )\n",
    "  (22): Linear(in_features=512, out_features=512, bias=False)\n",
    "  (23): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (24): ReLU()\n",
    "  (25): Linear(in_features=512, out_features=512, bias=False)\n",
    "  (26): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (27): ReLU()\n",
    "  (28): Linear(in_features=512, out_features=10, bias=True)\n",
    ")\n",
    "\n",
    "\n",
    "SGD(momentum=0.9, lr=0.1): 78.67 %\n",
    "Adam(lr=0.1): ~73.44 %\n",
    "ALL-CONV-AVGPOOL\n",
    "Sequential(\n",
    "  (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (2): ReLU()\n",
    "  (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (5): ReLU()\n",
    "  (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (8): ReLU()\n",
    "  (9): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (10): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (11): ReLU()\n",
    "  (12): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (13): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (14): ReLU()\n",
    "  (15): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (16): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (17): ReLU()\n",
    "  (18): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (19): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (20): ReLU()\n",
    "  (21): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (22): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (23): ReLU()\n",
    "  (24): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (25): AvgPool2d(kernel_size=(6, 6), stride=(6, 6), padding=0, ceil_mode=False, count_include_pad=True)\n",
    "  (26): Flatten(\n",
    "  )\n",
    ")\n",
    "\n",
    "lr=0.01 RESNET-18: 64.32 %\n",
    "\n",
    "SGD(lr=0.01, momentum=0.9): 86.28 %\n",
    "CONV+POOL_FC\n",
    "+ Dropout: 87.08 %\n",
    "+ additional dropout: 86.64 %\n",
    "Sequential(\n",
    "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
    "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (2): LeakyReLU(0.01)\n",
    "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (5): LeakyReLU(0.01)\n",
    "  (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
    "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (9): LeakyReLU(0.01)\n",
    "  (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (12): LeakyReLU(0.01)\n",
    "  (13): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
    "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (16): LeakyReLU(0.01)\n",
    "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (19): LeakyReLU(0.01)\n",
    "  (20): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
    "  (21): Flatten(\n",
    "  )\n",
    "  (22): Linear(in_features=4096, out_features=2048, bias=False)\n",
    "  (23): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (24): LeakyReLU(0.01)\n",
    "  (25): Linear(in_features=2048, out_features=1024, bias=False)\n",
    "  (26): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (27): LeakyReLU(0.01)\n",
    "  (28): Linear(in_features=1024, out_features=10, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
